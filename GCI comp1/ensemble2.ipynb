{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45724dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce6e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d393de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "passengers_ID = test_df['PassengerId']\n",
    "combine = [train_df,test_df]\n",
    "combine_df = pd.concat([train_df, test_df]).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f64a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://github.com/thisisjasonjafari/my-datascientise-handcode/raw/master/005-datavisualization/titanic.csv\"\n",
    "s=requests.get(url).content\n",
    "c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    " \n",
    "test_data_with_labels = c\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd179c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(test_data_with_labels['name']):\n",
    "    if '\"' in name:\n",
    "        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "for i, name in enumerate(test_data['Name']):\n",
    "    if '\"' in name:\n",
    "        test_data['Name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "survived = []\n",
    "\n",
    "for name in test_data['Name']:\n",
    "    survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6402d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(test_data_with_labels['name']):\n",
    "    if '\"' in name:\n",
    "        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "for i, name in enumerate(train_df['Name']):\n",
    "    if '\"' in name:\n",
    "        train_df['Name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "survived_train = []\n",
    "\n",
    "for name in train_df['Name']:\n",
    "    survived_train.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c2d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "passengers_ID = test_df['PassengerId']\n",
    "combine = [train_df,test_df]\n",
    "def Preprocessing(dataset):\n",
    "    #Titleの処理\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    #SibSpとParchの処理\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    #FamilySizeの分類\n",
    "    dataset['SmallFamily'] = dataset['FamilySize'].map(lambda s: 1if 2 <= s <= 4 else 0)\n",
    "    dataset['LargeFamily'] = dataset['FamilySize'].map(lambda s: 1 if  5 <=s else 0)\n",
    "    #Cabinの処理\n",
    "    cabin_only = dataset[[\"Cabin\"]].copy()\n",
    "    cabin_only[\"Cabin_Data\"] = cabin_only[\"Cabin\"].isnull().apply(lambda x: not x)\n",
    "    cabin_only[\"Deck\"] = cabin_only[\"Cabin\"].str.slice(0,1)\n",
    "    cabin_only[\"Deck\"] = cabin_only[\"Deck\"].fillna(\"N\")\n",
    "    cabin_only[\"Deck\"] = cabin_only['Deck'].replace('T','N')\n",
    "    dataset = pd.concat([ dataset,cabin_only[\"Deck\"]], axis=1)\n",
    "    \n",
    "    Ticket = []\n",
    "    for i in list(dataset.Ticket):\n",
    "        if not i.isdigit() :\n",
    "            Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n",
    "        else:\n",
    "            Ticket.append(\"X\")\n",
    "\n",
    "    dataset[\"Ticket\"] = Ticket\n",
    "    \n",
    "    dataset = dataset.drop(columns = ['Name', 'SibSp', 'Parch', 'Cabin','PassengerId','FamilySize'])\n",
    "#     # 欠損値の補完\n",
    "#     x = combine_df.loc[(combine_df['Pclass'] == 3) & \n",
    "#                (combine_df['Sex'] == 'male' ) &\n",
    "#                (combine_df['Embarked'] == 'S') & \n",
    "#                (combine_df['Title'] == 'Mr' ) &\n",
    "#                (combine_df['FamilySize'] == 1 ) &\n",
    "#                (combine_df['Fare'] < 10 ) &\n",
    "#                 (combine_df['Fare'] > 5 ) ,\n",
    "#                 'Fare']\n",
    "    # 欠損値を Embarked='S', Pclass=3 の平均値で補完\n",
    "    dataset.Fare.fillna(dataset.mean(), inplace=True)\n",
    "    #’Embarked’最頻値で補完\n",
    "    dataset.Embarked.fillna(dataset.Embarked.mode()[0], inplace=True)\n",
    "    \n",
    "    #dataset.Age.fillna(dataset.groupby(['Sex','Pclass','Title','Deck']).transform('median').Age, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "train_df = Preprocessing(train_df)\n",
    "test_df = Preprocessing(test_df)\n",
    "combine_df = pd.concat([train_df, test_df]).reset_index().drop(['index'], axis=1)\n",
    "test_df.Age.fillna(test_df.groupby(['Sex','Pclass','Title','Deck']).transform('median').Age, inplace=True)\n",
    "\n",
    "chang_list = []\n",
    "for i in test_df['Ticket']:\n",
    "    if not i in train_df['Ticket'].tolist():\n",
    "        chang_list.append(i)\n",
    "test_df['Ticket'] = test_df['Ticket'].replace(chang_list, 'X')\n",
    "chang_list = []\n",
    "for i in train_df['Ticket']:\n",
    "    if not i in test_df['Ticket'].tolist():\n",
    "        chang_list.append(i)\n",
    "train_df['Ticket'] = train_df['Ticket'].replace(chang_list, 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155396d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = Preprocessing(train_df)\n",
    "test_df = Preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e52cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCA3', 'STONOQ', 'AQ4', 'A', 'LP', 'AQ3']\n",
      "['SP', 'SOP', 'Fa', 'LINE', 'SWPP', 'SCOW', 'LINE', 'LINE', 'PPP', 'AS', 'SWPP', 'LINE', 'PPP', 'CASOTON']\n"
     ]
    }
   ],
   "source": [
    "chang_list = []\n",
    "for i in test_df['Ticket']:\n",
    "    if not i in train_df['Ticket'].tolist():\n",
    "        chang_list.append(i)\n",
    "test_df['Ticket'] = test_df['Ticket'].replace(chang_list, 'X')\n",
    "print(chang_list)\n",
    "chang_list = []\n",
    "for i in train_df['Ticket']:\n",
    "    if not i in test_df['Ticket'].tolist():\n",
    "        chang_list.append(i)\n",
    "train_df['Ticket'] = train_df['Ticket'].replace(chang_list, 'X')\n",
    "print(chang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b7e64997",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "Y_train = train_df['Perished'].values\n",
    "train_df = train_df.drop(columns='Perished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "56f5963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for dataset in [train_df,test_df]:\n",
    "    dataset['Sex'] = le.fit_transform(dataset['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "abcc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array( test_df )\n",
    "X_train = np.array( train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eccd57",
   "metadata": {},
   "source": [
    "# Gridsearch\n",
    "# 時間がかかるので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d449b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def Predict(model, test = test):\n",
    "    proba =  model.best_estimator_.predict_proba(test)\n",
    "    result = model.best_estimator_.predict(test)\n",
    "    return proba, result\n",
    "def show_score(model,X_train = X_train, test = test):\n",
    "    print('Best Parameters: {}'.format(model.best_params_))\n",
    "    print('CV Score: {}'.format(round(model.best_score_, 5)))\n",
    "    \n",
    "    result = model.best_estimator_.predict(test)\n",
    "    print('test_score: {}'.format(1  - (result == survived).sum() / len(survived)))\n",
    "    \n",
    "    bag_model=BaggingClassifier(base_estimator= model.best_estimator_)\n",
    "    bag_model.fit(X_train, Y_train)\n",
    "    result = bag_model.predict(test)\n",
    "    print('Bag_score: {}'.format( 1 - (result == survived).sum() / len(survived)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8e8f1",
   "metadata": {},
   "source": [
    "6/6\n",
    "Best Parameters: {'learning_rate': 0.13333333333333336, 'max_depth': 2, 'n_estimators': 171, 'num_leaves': 3}\n",
    "CV Score: 0.83898\n",
    "test_score: 0.784688995215311\n",
    "Bag_score: 0.7583732057416268\n",
    "CPU times: user 1h 49min 39s, sys: 15.9 s, total: 1h 49min 55s\n",
    "Wall time: 1min 43s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "4abfafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.13333333333333336, 'max_depth': 2, 'n_estimators': 171, 'num_leaves': 3}\n",
      "CV Score: 0.83898\n",
      "test_score: 0.784688995215311\n",
      "Bag_score: 0.7583732057416268\n",
      "CPU times: user 1h 49min 39s, sys: 15.9 s, total: 1h 49min 55s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#LightGBMはCategorical Featureの設定によって、one-hot encodingより良い精度を出す\n",
    "train_df_lgbm = train_df\n",
    "test_lgbm = test_df\n",
    "le = LabelEncoder()\n",
    "train_df_lgbm['Pclass'] = le.fit_transform(train_df_lgbm['Pclass'])\n",
    "train_df_lgbm['Sex'] = le.fit_transform(train_df_lgbm['Sex'])\n",
    "train_df_lgbm['Ticket'] = le.fit_transform(train_df_lgbm['Ticket'])\n",
    "train_df_lgbm['Title'] = le.fit_transform(train_df_lgbm['Title'])\n",
    "train_df_lgbm['Embarked'] = le.fit_transform(train_df_lgbm['Embarked'])\n",
    "train_df_lgbm['Deck'] = le.fit_transform(train_df_lgbm['Deck'])\n",
    "\n",
    "test_lgbm['Pclass'] = le.fit_transform(test_lgbm['Pclass'])\n",
    "test_lgbm['Sex'] = le.fit_transform(test_lgbm['Sex'])\n",
    "test_lgbm['Ticket'] = le.fit_transform(test_lgbm['Ticket'])\n",
    "test_lgbm['Title'] = le.fit_transform(test_lgbm['Title'])\n",
    "test_lgbm['Embarked'] = le.fit_transform(test_lgbm['Embarked'])\n",
    "test_lgbm['Deck'] = le.fit_transform(test_lgbm['Deck'])\n",
    "\n",
    "categorical_feature = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title',\n",
    "       'SmallFamily', 'LargeFamily', 'Deck']\n",
    "\n",
    "learning_rate = [float(x) for x in np.linspace( 0.1, 0.4,10)]\n",
    "n_estimators = [int(x) for x in np.linspace(10,300,10)]\n",
    "max_depth = [2,3]\n",
    "num_leaves = [3,4,5,6]\n",
    "reg_lambda = [float(x) for x in np.linspace( 1e-10, 1e-8, 10)]\n",
    "reg_alpha = [float(x) for x in np.linspace( 1e-9, 1e-8, 5)]\n",
    "min_gain_to_split = [int(x) for x in np.arange(1,15, 5)]\n",
    "lambda_l2 = [int(x) for x in np.arange(10,100, 10)]\n",
    "lambda_l1 = [int(x) for x in np.arange(10,100, 10)]\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth' :max_depth,\n",
    "    \"num_leaves\":num_leaves,\n",
    "    #'reg_lambda':reg_lambda,\n",
    "   #'reg_alpha':reg_alpha,\n",
    "    #'min_gain_to_split':min_gain_to_split,\n",
    "    #\"lambda_l1\":lambda_l1,\n",
    "    #\"lambda_l2\":lambda_l2,\n",
    "}\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "lgbm_gs = GridSearchCV(\n",
    "    lgbm, \n",
    "    param_grid_lgbm, \n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "lgbm_gs.fit(\n",
    "    train_df_lgbm,\n",
    "    Y_train,\n",
    "    categorical_feature = categorical_feature\n",
    ")\n",
    "\n",
    "show_score( lgbm_gs , X_train=train_df_lgbm , test = test_lgbm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "05ef8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_lgbm, result_lgbm = Predict(lgbm_gs, test = test_lgbm)\n",
    "result_df = pd.DataFrame()\n",
    "result_df['lgbm'] =result_lgbm\n",
    "result_df.to_csv('result6_6_15:00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dacb3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": passengers_ID,\n",
    "        \"Perished\": result_lgbm\n",
    "    })\n",
    "submission.to_csv('submission6_6_15:00',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "8b6d64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": passengers_ID,\n",
    "        \"Perished\": result_lgbm\n",
    "    })\n",
    "submission.to_csv('submission6_6_15:00',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "812affe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(dataset, i, ):\n",
    "    dummies = pd.get_dummies(dataset[i])\n",
    "    dataset = pd.concat([ dataset, dummies], axis=1)\n",
    "    dataset = dataset.drop(i, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "53642959",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Ticket'] = test_df['Ticket'].replace('SCA4', 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "8894d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dummies(train_df, 'Title')\n",
    "train_df = dummies(train_df, 'Embarked')\n",
    "train_df = dummies(train_df, 'Deck')\n",
    "train_df = dummies(train_df, 'Ticket')\n",
    "test_df = dummies(test_df, 'Title')\n",
    "test_df = dummies(test_df, 'Embarked')\n",
    "test_df = dummies(test_df, 'Deck')\n",
    "test_df = dummies(test_df, 'Ticket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "74521140",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['F', 'G', 'A4', 'C', 'CA', 'FC', 'FCC', 'PP', 'SC', 'SCAH', 'SCPARIS', 'SCParis', 'SOC', 'SOPP', 'SOTONO2', 'SOTONOQ', 'STONO2', 'WC', 'WEP'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [466]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m drop_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCAH\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCPARIS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCParis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOPP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOTONO2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOTONOQ\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTONO2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWEP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vis/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vis/lib/python3.10/site-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vis/lib/python3.10/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/vis/lib/python3.10/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/vis/lib/python3.10/site-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['F', 'G', 'A4', 'C', 'CA', 'FC', 'FCC', 'PP', 'SC', 'SCAH', 'SCPARIS', 'SCParis', 'SOC', 'SOPP', 'SOTONO2', 'SOTONOQ', 'STONO2', 'WC', 'WEP'] not found in axis\""
     ]
    }
   ],
   "source": [
    "drop_list = ['F', 'G', 'A4', 'C', 'CA', 'FC', 'FCC', 'PP', 'SC', 'SCAH',\n",
    "       'SCPARIS', 'SCParis', 'SOC', 'SOPP', 'SOTONO2', 'SOTONOQ',\n",
    "       'STONO2', 'WC', 'WEP']\n",
    "train_df = train_df.drop(columns=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "02b77008",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "d8102613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_df)\n",
    "test = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d937a07",
   "metadata": {},
   "source": [
    "6/6\n",
    "Best Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 60}\n",
    "CV Score: 0.81661\n",
    "test_score: 0.7870813397129186\n",
    "Bag_score: 0.777511961722488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "066f04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 15}\n",
      "CV Score: 0.82917\n",
      "test_score: 0.777511961722488\n",
      "Bag_score: 0.7918660287081339\n",
      "CPU times: user 1min 5s, sys: 112 ms, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_estimators = [int(x) for x in np.arange(10,100, 5)]\n",
    "max_features = ['auto'] \n",
    "max_depth = [int(x) for x in np.arange(2,7)]\n",
    "#max_depth.append(None)\n",
    "min_samples_split = [int(x) for x in np.arange(2,5)]\n",
    "min_samples_leaf = [int(x) for x in np.arange(2,5)]\n",
    "bootstrap = [True] # False\n",
    "criterion = ['gini']\n",
    "\n",
    "# Create the random grid\n",
    "param_grid_rfc = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               #'bootstrap': bootstrap,\n",
    "                #'criterion' :criterion,\n",
    "                 }\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=SEED)\n",
    "rfc_gs = GridSearchCV( rfc, param_grid_rfc, cv=5)\n",
    "rfc_gs.fit(X_train, Y_train, )\n",
    "\n",
    "show_score( rfc_gs , X_train=X_train, test = test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363b48f",
   "metadata": {},
   "source": [
    "6/6\n",
    "Best Parameters: {'learning_rate': 0.15250000000000002, 'max_depth': 3, 'n_estimators': 177}\n",
    "CV Score: 0.83757\n",
    "test_score: 0.7559808612440191\n",
    "Bag_score: 0.7631578947368421\n",
    "CPU times: user 1h 15min 53s, sys: 29.5 s, total: 1h 16min 23s\n",
    "Wall time: 1min 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "11380a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.15250000000000002, 'max_depth': 3, 'n_estimators': 177}\n",
      "CV Score: 0.83757\n",
      "test_score: 0.7559808612440191\n",
      "Bag_score: 0.7631578947368421\n",
      "CPU times: user 1h 15min 53s, sys: 29.5 s, total: 1h 16min 23s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#時間がかかるので注意\n",
    "max_depth = [int(x) for x in np.arange(2,6)]\n",
    "n_estimators = [int(x) for x in np.linspace(50,300,10)]\n",
    "learning_rate = [ float(x) for x in np.linspace(0.1, 0.2,5) ]\n",
    "gamma = [ float(x) for x in np.linspace(1e-8, 1.0, 10) ]\n",
    "eta = [ float(x) for x in np.linspace(1e-8, 1.0, 10) ]\n",
    "param_grid_xgb = {\n",
    "    'max_depth': max_depth,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    #'gamma': gamma,\n",
    "    #'eta':eta,\n",
    "    \n",
    "}\n",
    "\n",
    "xgbc = xgb.XGBClassifier(\n",
    "    random_state=SEED,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "xgb_gs = GridSearchCV(\n",
    "    xgbc, \n",
    "    param_grid_xgb, \n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "xgb_gs.fit(X_train, Y_train)\n",
    "\n",
    "show_score( xgb_gs , X_train=X_train , test = test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_xgbc, result_xgbc = Predict(xgb_gs)\n",
    "result_df['xgbc'] = result_xgbc\n",
    "result_df.to_csv('result6_6_15:00.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbf4fb",
   "metadata": {},
   "source": [
    "# Bsgging Ensemble\n",
    "lgbm_gs, cat_gs, xgb_gsのは結果がいい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model=BaggingClassifier(base_estimator= lgbm_gs.best_estimator_)\n",
    "bag_model.fit(X_train, Y_train)\n",
    "result_lgbm = bag_model.predict(test)\n",
    "result_df['lgbm'] = result_lgbm\n",
    "result_df.to_csv('result6_1_3:00.csv')\n",
    "\n",
    "bag_model=BaggingClassifier(base_estimator= xgb_gs.best_estimator_)\n",
    "bag_model.fit(X_train, Y_train)\n",
    "result_xgbc = bag_model.predict(test)\n",
    "result_df['xgbc'] = result_xgbc\n",
    "result_df.to_csv('result6_1_3:00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39064b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.mode(axis=1)[0].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model=BaggingClassifier(base_estimator= lgbm_gs.best_estimator_)\n",
    "bag_model.fit(X_train, Y_train)\n",
    "result_lgbm = bag_model.predict(test)\n",
    "result_df['lgbm'] = result_lgbm\n",
    "result_df.to_csv('result6_1_3:00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aae815",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": passengers_ID,\n",
    "        \"Perished\": result_df.mode(axis=1)[0].values.astype(int)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e55a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission6_2_15:00',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
